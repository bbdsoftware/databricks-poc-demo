{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51beee35",
   "metadata": {},
   "source": [
    "MLflow 3.0 traditional ML example\n",
    "\n",
    "This notebook first runs a model training job, which is tracked as an MLflow Run, to produce a trained model. The model is tracked as an MLflow Logged Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf031ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install mlflow>=3.0 --upgrade\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab8f02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.entities import Dataset\n",
    "\n",
    "# Setup the experiment\n",
    "mlflow.set_experiment(\"/Workspace/Shared/iris\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebeba3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to compute metrics\n",
    "def compute_metrics(actual, predicted):\n",
    "    rmse = mean_squared_error(actual, predicted)\n",
    "    mae = mean_absolute_error(actual, predicted)\n",
    "    r2 = r2_score(actual, predicted)\n",
    "    return rmse, mae, r2\n",
    "\n",
    "\n",
    "# Load Iris dataset and prepare the DataFrame\n",
    "iris = load_iris()\n",
    "iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "iris_df['quality'] = (iris.target == 2).astype(float)  # Create a binary target for simplicity\n",
    "\n",
    "# Split into training and testing datasets\n",
    "train_df, test_df = train_test_split(iris_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Start a run to represent the training job\n",
    "with mlflow.start_run() as training_run:\n",
    "    # Load the training dataset with MLflow. We will link training metrics to this dataset.\n",
    "    train_dataset: Dataset = mlflow.data.from_pandas(train_df, name=\"train\")\n",
    "    train_x = train_dataset.df.drop([\"quality\"], axis=1)\n",
    "    train_y = train_dataset.df[[\"quality\"]]\n",
    "\n",
    "    # Fit a model to the training dataset\n",
    "    lr = ElasticNet(alpha=0.5, l1_ratio=0.5, random_state=42)\n",
    "    lr.fit(train_x, train_y)\n",
    "\n",
    "    # Log the model, specifying its ElasticNet parameters (alpha, l1_ratio)\n",
    "    # As a new feature, the LoggedModel entity is linked to its name and params\n",
    "    model_info = mlflow.sklearn.log_model(\n",
    "        sk_model=lr,\n",
    "        name=\"elasticnet\",\n",
    "        params={\n",
    "            \"alpha\": 0.5,\n",
    "            \"l1_ratio\": 0.5,\n",
    "        },\n",
    "        input_example = train_x\n",
    "    )\n",
    "\n",
    "    # Inspect the LoggedModel and its properties\n",
    "    logged_model = mlflow.get_logged_model(model_info.model_id)\n",
    "    print(logged_model.model_id, logged_model.params)\n",
    "\n",
    "    # Evaluate the model on the training dataset and log metrics\n",
    "    # These metrics are now linked to the LoggedModel entity\n",
    "    predictions = lr.predict(train_x)\n",
    "    (rmse, mae, r2) = compute_metrics(train_y, predictions)\n",
    "    mlflow.log_metrics(\n",
    "        metrics={\n",
    "            \"rmse\": rmse,\n",
    "            \"r2\": r2,\n",
    "            \"mae\": mae,\n",
    "        },\n",
    "        model_id=logged_model.model_id,\n",
    "        dataset=train_dataset\n",
    "    )\n",
    "\n",
    "    # Inspect the LoggedModel, now with metrics\n",
    "    logged_model = mlflow.get_logged_model(model_info.model_id)\n",
    "    print(logged_model.model_id, logged_model.metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f91d039",
   "metadata": {},
   "source": [
    "Some time later, when you get a new evaluation dataset based on the latest production data, you can run a new model evaluation job, which is tracked as a new MLflow Run, to measure the performance of the model on this new dataset.\n",
    "\n",
    "This example produces two MLflow Runs (training_run and evaluation_run) and one MLflow LoggedModel (elasticnet). From the resulting LoggedModel, you can see all of the parameters and metadata, as well as all of the metrics linked from the training and evaluation runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fc015b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a run to represent the test dataset evaluation job\n",
    "with mlflow.start_run() as evaluation_run:\n",
    "  # Load the test dataset with MLflow. We will link test metrics to this dataset.\n",
    "  test_dataset: mlflow.entities.Dataset = mlflow.data.from_pandas(test_df, name=\"test\")\n",
    "  test_x = test_dataset.df.drop([\"quality\"], axis=1)\n",
    "  test_y = test_dataset.df[[\"quality\"]]\n",
    "\n",
    "  # Load the model\n",
    "  model = mlflow.sklearn.load_model(f\"models:/{logged_model.model_id}\")\n",
    "\n",
    "  # Evaluate the model on the training dataset and log metrics, linking to model\n",
    "  predictions = model.predict(test_x)\n",
    "  (rmse, mae, r2) = compute_metrics(test_y, predictions)\n",
    "  mlflow.log_metrics(\n",
    "    metrics={\n",
    "      \"rmse\": rmse,\n",
    "      \"r2\": r2,\n",
    "      \"mae\": mae,\n",
    "    },\n",
    "    dataset=test_dataset,\n",
    "    model_id=logged_model.model_id\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b81006f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mlflow.get_logged_model(logged_model.model_id).to_dictionary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782dde4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You must have `USE CATALOG` privileges on the catalog, and you must have `USE SCHEMA` privileges on the schema.\n",
    "# If necessary, change the catalog and schema name here.\n",
    "\n",
    "CATALOG = \"sandbox\"\n",
    "SCHEMA = \"things\"\n",
    "MODEL = \"iris-model\"\n",
    "MODEL_NAME = f\"{CATALOG}.{SCHEMA}.{MODEL}\"\n",
    "\n",
    "uc_model_version = mlflow.register_model(model_info.model_uri, name=MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230bbdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Now you can view the model version and all centralized performance data on the model version page in Unity Catalog. You can also get the same information using the API as shown in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7fdd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model version\n",
    "from mlflow import MlflowClient\n",
    "client = MlflowClient()\n",
    "model_version = client.get_model_version(name=MODEL_NAME, version=uc_model_version.version)\n",
    "print(model_version)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
